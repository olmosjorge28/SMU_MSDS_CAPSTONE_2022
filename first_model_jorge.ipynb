{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RMSprop' from 'keras.optimizers' (/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-383d14fe8139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RMSprop' from 'keras.optimizers' (/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import torch\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.functional import accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Conv1D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "# from keras.optimizers import RMSprop, adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "day1_dataset_10min = pd.read_csv('Data Slices/day1_dataset_10min_10Aug2022.csv')\n",
    "day1_dataset_30min = pd.read_csv('Data Slices/day1_dataset_30min_10Aug2022.csv')\n",
    "day1_dataset_60min = pd.read_csv('Data Slices/day1_dataset_60min_10Aug2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_and_y(df, x_columns, y_columns):\n",
    "    X = df[x_columns]\n",
    "    y = df[y_columns]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_sequences(X, y, feature_columns, sequence_dict = {}):\n",
    "    for collection, group in X.groupby(\"collection\"):\n",
    "        features = group[group.columns.intersection(feature_columns)]\n",
    "        blacklist = y[y.collection == collection].iloc[0].blacklisted\n",
    "        if sequence_dict.get(collection):\n",
    "            sequence_dict[collection][0].append(features)\n",
    "        else:\n",
    "            sequence_dict[collection] = ([features], blacklist)\n",
    "    return sequence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_sequences(X, y, feature_columns, sequence_dict = {}):\n",
    "    for collection, group in X.groupby(\"collection\"):\n",
    "        features = group[group.columns.intersection(feature_columns)]\n",
    "        blacklist = y[y.collection == collection].iloc[0].blacklisted\n",
    "        if sequence_dict.get(collection):\n",
    "            sequence_dict[collection][0].append(features)\n",
    "        else:\n",
    "            sequence_dict[collection] = ([features], blacklist)\n",
    "    return sequence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(sequences, columns = ['Price_USD', 'Price_Crypto', 'volume', 'densities', 'vertex_count', \n",
    "                              'edge_count','vertext_edge_ratio'], fitted_scaler = None):\n",
    "    collection_blacklist_dict = {sequence[0]:sequence[2] for sequence in sequences}\n",
    "    concat_df = pd.concat([sequence[1] for sequence in sequences])\n",
    "    if fitted_scaler is None:\n",
    "        fitted_scaler = StandardScaler()\n",
    "        fitted_scaler.fit(concat_df[columns])\n",
    "    concat_df[columns] = fitted_scaler.transform(concat_df[columns])\n",
    "    return  [(collection, group[columns], collection_blacklist_dict[collection]) \n",
    "             for collection, group in concat_df.groupby(\"collection\")] , fitted_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sequences(all_df):\n",
    "    sequence_dict = {}\n",
    "    for df in all_df: \n",
    "        df = df.drop(['Unnamed: 0'], axis=1)\n",
    "        x_columns =  ['Datetime_updated_seconds','Price_USD','Price_Crypto','volume','densities',\n",
    "                   'vertex_count','edge_count','collection']\n",
    "        y_columns = ['blacklisted','collection']\n",
    "        feature_columns=['Price_USD','Price_Crypto','volume',\n",
    "                         'densities', 'vertex_count','edge_count', 'collection', 'vertext_edge_ratio']\n",
    "        X, y= get_X_and_y(df, x_columns, y_columns)\n",
    "        X['vertext_edge_ratio'] = X['vertex_count']/X['edge_count']\n",
    "        sequence_dict = get_daily_sequences(X, y, feature_columns, sequence_dict)\n",
    "    return sequence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_all_train_test_sequences(all_df):\n",
    "    train_test_sequences = []\n",
    "    all_sequence_dict = get_all_sequences(all_df)\n",
    "        \n",
    "    train_sequences, test_sequences = train_test_split(list(all_sequence_dict.items()), test_size = 0.3)\n",
    "#     for train_s\n",
    "#         scaled_train_sequences, scaler = scale_dataset(train_sequences)\n",
    "#         scaled_test_sequences ,  _ = scale_dataset(test_sequences, fitted_scale r= scaler)\n",
    "    return train_sequences, test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f9d6ae458486>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['vertext_edge_ratio'] = X['vertex_count']/X['edge_count']\n"
     ]
    }
   ],
   "source": [
    "train_sequences, test_sequences = retrieve_all_train_test_sequences([day1_dataset_10min, day1_dataset_30min, day1_dataset_60min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_dataset_dict = {}\n",
    "scaled_test_dataset_dict = {}\n",
    "scaled_train_dataset= []\n",
    "scaled_test_dataset= []\n",
    "length = 3\n",
    "for n in range(length):\n",
    "    train_scaled, scaler = scale_dataset([(sequence[0], sequence[1][0][n], sequence[1][1]) for sequence in\n",
    "                                          train_sequences])\n",
    "    test_scaled, _ = scale_dataset([(sequence[0], sequence[1][0][n], sequence[1][1]) for sequence in\n",
    "                                    test_sequences],\n",
    "                  fitted_scaler= scaler)\n",
    "    scaled_train_dataset.append(train_scaled)\n",
    "    scaled_test_dataset.append(test_scaled)\n",
    "    \n",
    "for scale_sequence in scaled_train_dataset:\n",
    "    for collection in scale_sequence:\n",
    "        if scaled_train_dataset_dict.get(collection[0]):\n",
    "            scaled_train_dataset_dict[collection[0]][0].append(collection[1])\n",
    "        else:\n",
    "            scaled_train_dataset_dict[collection[0]] = ([collection[1]], collection[2])\n",
    "\n",
    "    \n",
    "for scale_sequence in scaled_test_dataset:\n",
    "    for collection in scale_sequence:\n",
    "        if scaled_test_dataset_dict.get(collection[0]):\n",
    "            scaled_test_dataset_dict[collection[0]][0].append(collection[1])\n",
    "        else:\n",
    "            scaled_test_dataset_dict[collection[0]] = ([collection[1]], collection[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(scaled_train_dataset_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(scaled_test_dataset_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(scaled_test_dataset_dict.values())[1][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(scaled_train_dataset_dict.values())[1][0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = list(scaled_train_dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = list(scaled_test_dataset_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_SHAPE1  = train_values[0][0][0].shape\n",
    "MODEL_SHAPE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_SHAPE2  = train_values[0][0][1].shape\n",
    "MODEL_SHAPE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_SHAPE3 = train_values[0][0][2].shape\n",
    "MODEL_SHAPE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS1 = 10\n",
    "NUM_FILTERS2 = 5\n",
    "NUM_FILTERS3 = 3\n",
    "K_SIZE1 = 8\n",
    "K_SIZE2 = 6\n",
    "K_SIZE3 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(shape, k_size = K_SIZE1, num_filters = NUM_FILTERS1):\n",
    "    print(\"base model shape\", shape)\n",
    "    input_seq = Input(shape=shape)\n",
    "    nb_filters = num_filters\n",
    "    convolved = Conv1D(num_filters, k_size, padding=\"same\", activation=\"relu\")(input_seq)\n",
    "    processed = GlobalMaxPooling1D()(convolved)\n",
    "    compressed = Dense(50, activation=\"relu\")(processed)\n",
    "    compressed = Dropout(0.3)(compressed)\n",
    "    model = Model(inputs=input_seq, outputs=compressed)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_model(shape1, shape2, shape3):\n",
    "    shape1_input = Input(shape=shape1, name='input1')\n",
    "    shape2_input = Input(shape=shape2, name='input2')\n",
    "    shape3_input = Input(shape=shape3, name='input3')\n",
    "    shape1_model = get_base_model(shape1, k_size = K_SIZE1, num_filters=NUM_FILTERS1)\n",
    "    shape2_model = get_base_model(shape2, k_size = K_SIZE2, num_filters=NUM_FILTERS2)\n",
    "    shape3_model = get_base_model(shape3, k_size = K_SIZE3, num_filters=NUM_FILTERS3)\n",
    "    \n",
    "    embedding1 = shape1_model(shape1_input)\n",
    "    embedding2 = shape2_model(shape2_input)\n",
    "    embedding3 = shape3_model(shape3_input)\n",
    "    merged = Concatenate()([embedding1, embedding2, embedding3])\n",
    "    # todo: add hidden layers\n",
    "    out = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[shape1_input, shape2_input, shape3_input], outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(x_train, y_train, x_test, y_test):\n",
    "    es = keras.callbacks.EarlyStopping(min_delta=0.00001, patience=10)\n",
    "    #data_set = tf.data.Dataset.from_tensor_slices(  (x_train ,y_train) )\n",
    "    model = main_model(MODEL_SHAPE1, MODEL_SHAPE2, MODEL_SHAPE3)\n",
    "    model.compile(loss='binary_crossentropy', # categorical_crossentropy\n",
    "                          optimizer='rmsprop', #sgd, nadam, adam, rmsprop\n",
    "                          metrics=['binary_accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),\n",
    "                                   tf.keras.metrics.AUC(curve='PR')])\n",
    "    model.summary()\n",
    "    print('SIZE OF TRAINING DATA', len(x_train))\n",
    "    model_hist = model.fit(x_train, y_train,\n",
    "                               validation_data=(x_test, y_test),\n",
    "                               batch_size=100, epochs=1000, \n",
    "                           callbacks=[es]\n",
    "                          )\n",
    "    return model_hist, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = [sequence[0][0] for sequence in train_values]\n",
    "x_train_2 = [sequence[0][1] for sequence in train_values]\n",
    "x_train_3 = [sequence[0][2] for sequence in train_values]\n",
    "y_train = [sequence[1] for sequence in train_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.stack(x_train_1)\n",
    "X_train2 = np.stack(x_train_2)\n",
    "X_train3 = np.stack(x_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train1.reshape(696,144,7)\n",
    "X_train2 = X_train2.reshape(696,48,7)\n",
    "X_train3 = X_train3.reshape(696,24,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_1 = [sequence[0][0] for sequence in test_values]\n",
    "x_test_2 = [sequence[0][1] for sequence in test_values]\n",
    "x_test_3 = [sequence[0][2] for sequence in test_values]\n",
    "y_test = [sequence[1] for sequence in train_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = np.stack(x_test_1)\n",
    "X_test2 = np.stack(x_test_2)\n",
    "X_test3 = np.stack(x_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = X_test1.reshape(299,144,7)\n",
    "X_test2 = X_test2.reshape(299,48,7)\n",
    "X_test3 = X_test3.reshape(299,24,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = [sequence[0].to_numpy() for sequence in scaled_train_sequences]\n",
    "# y_train = [sequence[1] for sequence in scaled_train_sequences]\n",
    "# X_test = [sequence[0].to_numpy() for sequence in scaled_test_sequences]\n",
    "# y_test = [sequence[1] for sequence in scaled_test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model shape (144, 7)\n",
      "base model shape (48, 7)\n",
      "base model shape (24, 7)\n",
      "Model: \"model_91\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_139 (InputLayer)          [(None, 144, 7)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_140 (InputLayer)          [(None, 48, 7)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_141 (InputLayer)          [(None, 24, 7)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_88 (Functional)           (None, 50)           1120        input_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_89 (Functional)           (None, 50)           515         input_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_90 (Functional)           (None, 50)           266         input_141[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 150)          0           model_88[0][0]                   \n",
      "                                                                 model_89[0][0]                   \n",
      "                                                                 model_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 1)            151         concatenate_23[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,052\n",
      "Trainable params: 2,052\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "SIZE OF TRAINING DATA 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-c6551081cc28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-0d8b4721df7d>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SIZE OF TRAINING DATA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     model_hist = model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1135\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    974\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \"input: {}, {}\".format(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})"
     ]
    }
   ],
   "source": [
    "model_hist, model = run_model([X_train1, X_train2, X_train3], y_train, [X_test1, X_test2, X_test3],  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model shape (144, 7)\n",
      "base model shape (48, 7)\n",
      "base model shape (24, 7)\n",
      "(696, 144, 7)\n"
     ]
    }
   ],
   "source": [
    "a_model = run_model([X_train1, X_train2, X_train3], y_train, [X_test1, X_test2, X_test3],  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-24c9dce08bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1135\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    974\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \"input: {}, {}\".format(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(min_delta=0.00001, patience=10)\n",
    "model_hist = a_model.fit([X_train1, X_train2, X_train3], y_train,batch_size=50, epochs=1000, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(all_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[prediction <=0.5] = 0\n",
    "prediction[prediction >0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pred[0] for pred in prediction.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history, title=None):\n",
    "    ''' Plot the training curves for loss and accuracy given a model history\n",
    "    '''\n",
    "    # find the minimum loss epoch\n",
    "    minimum = np.min(history.history['val_loss'])\n",
    "    min_loc = np.where(minimum == history.history['val_loss'])[0]\n",
    "    # get the vline y-min and y-max\n",
    "    loss_min, loss_max = (min(history.history['val_loss'] + history.history['loss']),\n",
    "                          max(history.history['val_loss'] + history.history['loss']))\n",
    "    acc_min, acc_max = (min(history.history['val_binary_accuracy'] + history.history['binary_accuracy']),\n",
    "                        max(history.history['val_binary_accuracy'] + history.history['binary_accuracy']))\n",
    "    # create figure\n",
    "    fig, ax = plt.subplots(ncols=2, figsize = (15,7))\n",
    "    fig.suptitle(title)\n",
    "    index = np.arange(1, len(history.history['binary_accuracy']) + 1)\n",
    "    # plot the loss and validation loss\n",
    "    ax[0].plot(index, history.history['loss'], label = 'loss')\n",
    "    ax[0].plot(index, history.history['val_loss'], label = 'val_loss')\n",
    "    ax[0].vlines(min_loc + 1, loss_min, loss_max, label = 'min_loss_location')\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].legend()\n",
    "    # plot the accuracy and validation accuracy\n",
    "    ax[1].plot(index, history.history['binary_accuracy'], label = 'accuracy')\n",
    "    ax[1].plot(index, history.history['val_binary_accuracy'], label = 'val_accuracy')\n",
    "    ax[1].vlines(min_loc + 1, acc_min, acc_max, label = 'min_loss_location')\n",
    "    ax[1].set_title('Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves_auc(history, title=None):\n",
    "    ''' Plot the training curves for loss and accuracy given a model history\n",
    "    '''\n",
    "    # find the minimum loss epoch\n",
    "    minimum = np.min(history.history['val_loss'])\n",
    "    min_loc = np.where(minimum == history.history['val_loss'])[0]\n",
    "    # get the vline y-min and y-max\n",
    "    loss_min, loss_max = (min(history.history['val_loss'] + history.history['loss']),\n",
    "                          max(history.history['val_loss'] + history.history['loss']))\n",
    "    acc_min, acc_max = (min(history.history['val_auc'] + history.history['auc']),\n",
    "                        max(history.history['val_auc'] + history.history['auc']))\n",
    "    # create figure\n",
    "    fig, ax = plt.subplots(ncols=2, figsize = (15,7))\n",
    "    fig.suptitle(title)\n",
    "    index = np.arange(1, len(history.history['auc']) + 1)\n",
    "    # plot the loss and validation loss\n",
    "    ax[0].plot(index, history.history['loss'], label = 'loss')\n",
    "    ax[0].plot(index, history.history['val_loss'], label = 'val_loss')\n",
    "    ax[0].vlines(min_loc + 1, loss_min, loss_max, label = 'min_loss_location')\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].legend()\n",
    "    # plot the accuracy and validation accuracy\n",
    "    ax[1].plot(index, history.history['auc'], label = 'auc')\n",
    "    ax[1].plot(index, history.history['val_auc'], label = 'val_auc')\n",
    "    ax[1].vlines(min_loc + 1, acc_min, acc_max, label = 'min_loss_location')\n",
    "    ax[1].set_title('AUC')\n",
    "    ax[1].set_ylabel('auc')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(history=model_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves_auc(history=model_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
